{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import time\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import load_pickle, dump_pickle, get_feature_value, feature_spearmanr, feature_target_spearmanr, addCrossFeature, calibration\n",
    "from utils import raw_data_path, feature_data_path, cache_pkl_path, analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_path = feature_data_path + 'all_data_all_features.pkl'\n",
    "all_data = load_pickle(all_data_path)\n",
    "\n",
    "target = 'is_trade'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "226"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [\n",
    "    # 预处理后的基础特征----------------------------------------------\n",
    "#     'instance_id',\n",
    "     'item_id',\n",
    "     'item_brand_id',\n",
    "     'item_city_id',\n",
    "     'item_price_level',\n",
    "     'item_sales_level',\n",
    "     'item_collected_level',\n",
    "     'item_pv_level',\n",
    "     'user_id',\n",
    "     'user_gender_id',\n",
    "     'user_age_level',\n",
    "     'user_occupation_id',\n",
    "     'user_star_level',\n",
    "     'context_id',\n",
    "     'context_timestamp',\n",
    "     'context_page_id',\n",
    "     'shop_id',\n",
    "     'shop_review_num_level',\n",
    "     'shop_review_positive_rate',\n",
    "     'shop_star_level',\n",
    "     'shop_score_service',\n",
    "     'shop_score_delivery',\n",
    "     'shop_score_description',\n",
    "#     'is_trade',\n",
    "     'day',\n",
    "     'hour',\n",
    "#     'minute',\n",
    "     'category2_label',\n",
    "     'item_property_list0',\n",
    "     'item_property_list1',\n",
    "     'item_property_list2',\n",
    "     'item_property_list3',\n",
    "     'item_property_list4',\n",
    "     'item_property_list5',\n",
    "     'item_property_list6',\n",
    "     'item_property_list7',\n",
    "    #  2_1处理后的特征\n",
    "    #  生成用户对当天属性的点击量\n",
    "    #     ['item_id', 'item_brand_id', 'item_city_id', 'category2_label','item_price_level','item_sales_level',\n",
    "    #     'item_collected_level', 'item_pv_level',\n",
    "    #     'context_page_id',\n",
    "    #     'shop_id', 'shop_review_num_level', 'shop_star_level',]\n",
    "     'user_item_id_click_day',\n",
    "     'user_item_brand_id_click_day',\n",
    "#     'user_item_city_id_click_day',\n",
    "     'user_category2_label_click_day',\n",
    "     'user_item_price_level_click_day',\n",
    "     'user_item_sales_level_click_day',\n",
    "#     'user_item_collected_level_click_day',\n",
    "#     'user_item_pv_level_click_day',\n",
    "     'user_context_page_id_click_day',\n",
    "     'user_shop_id_click_day',\n",
    "#     'user_shop_review_num_level_click_day',\n",
    "#     'user_shop_star_level_click_day',\n",
    "    \n",
    "    #  2_5处理后的特征\n",
    "    #  user_id前一天点击某某某的数量\n",
    "    #  ['item_id', 'item_brand_id', 'shop_id', 'category2_label',]\n",
    "     'user_id_item_id_day_I',\n",
    "     'user_id_item_id_day_C',\n",
    "     'user_id_item_brand_id_day_I',\n",
    "     'user_id_item_brand_id_day_C',\n",
    "     'user_id_shop_id_day_I',\n",
    "     'user_id_shop_id_day_C',\n",
    "     'user_id_category2_label_day_I',\n",
    "     'user_id_category2_label_day_C',\n",
    "     'user_id_item_price_level_day_I',\n",
    "     'user_id_item_price_level_day_C',\n",
    "    #  2_5处理后的特征\n",
    "    #  user_id历史点击某某某的数量\n",
    "    #  ['item_id', 'item_brand_id', 'shop_id', 'category2_label',]\n",
    "     'user_id_item_id_history_I',\n",
    "     'user_id_item_id_history_C',\n",
    "     'user_id_item_brand_id_history_I',\n",
    "     'user_id_item_brand_id_history_C',\n",
    "     'user_id_shop_id_history_I',\n",
    "     'user_id_shop_id_history_C',\n",
    "     'user_id_category2_label_history_I',\n",
    "     'user_id_category2_label_history_C',\n",
    "     'user_id_item_price_level_history_I',\n",
    "     'user_id_item_price_level_history_C',\n",
    "    \n",
    "    #  2_1处理后的特征\n",
    "    #  生成用户对当天当小时属性的点击量\n",
    "    #     ['item_id', 'item_brand_id', 'item_city_id', 'category2_label','item_price_level','item_sales_level',\n",
    "    #     'item_collected_level', 'item_pv_level',\n",
    "    #     'context_page_id',\n",
    "    #     'shop_id', 'shop_review_num_level', 'shop_star_level',]\n",
    "     'user_item_id_click_hour',\n",
    "     'user_item_brand_id_click_hour',\n",
    "#     'user_item_city_id_click_hour',\n",
    "     'user_category2_label_click_hour',\n",
    "     'user_item_price_level_click_hour',\n",
    "     'user_item_sales_level_click_hour',\n",
    "#     'user_item_collected_level_click_hour',\n",
    "#     'user_item_pv_level_click_hour',\n",
    "     'user_context_page_id_click_hour',\n",
    "     'user_shop_id_click_hour',\n",
    "#     'user_shop_review_num_level_click_hour',\n",
    "#     'user_shop_star_level_click_hour',\n",
    "    #  2_1处理后的特征\n",
    "    #  生成用户对单一特征点击数据的统计特征\n",
    "     'user_item_id_click_day_mean',\n",
    "     'user_item_id_click_day_min',\n",
    "     'user_item_id_click_day_max',\n",
    "     'user_item_brand_id_click_day_mean',\n",
    "     'user_item_brand_id_click_day_min',\n",
    "     'user_item_brand_id_click_day_max',\n",
    "     'user_shop_id_click_day_mean',\n",
    "     'user_shop_id_click_day_min',\n",
    "     'user_shop_id_click_day_max',\n",
    "     'user_category2_label_click_day_mean',\n",
    "     'user_category2_label_click_day_min',\n",
    "     'user_category2_label_click_day_max',\n",
    "    \n",
    "    #  2_2处理后的特征\n",
    "    #  生成单一特征，日点击量的统计特征    stats_feature = ['user_id', 'item_id', 'item_brand_id', 'shop_id']\n",
    "     'user_id_click_day_mean',\n",
    "     'user_id_click_day_max',\n",
    "     'user_id_click_day_min',\n",
    "     'item_id_click_day_mean',\n",
    "     'item_id_click_day_max',\n",
    "     'item_id_click_day_min',\n",
    "     'item_brand_id_click_day_mean',\n",
    "     'item_brand_id_click_day_max',\n",
    "     'item_brand_id_click_day_min',\n",
    "     'shop_id_click_day_mean',\n",
    "     'shop_id_click_day_max',\n",
    "     'shop_id_click_day_min',\n",
    "    \n",
    "    #  2_3处理后的特征\n",
    "    #  生成用户日点击时间差特征\n",
    "     'user_click_rank_day',\n",
    "#     'user_first_click_day',\n",
    "#     'user_last_click_day',\n",
    "     'user_click_interval_first_day',\n",
    "     'user_click_interval_last_day',\n",
    "#     'user_click_interval_diff_day',\n",
    "#     'user_click_interval_prob',\n",
    "     'time_gap_before',\n",
    "     'time_gap_after',\n",
    "     'user_click_true_rank_day',\n",
    "    #  2_3处理后的特征\n",
    "    #  生成用户全局击时间差特征\n",
    "     'user_click_interval_mean_hour',\n",
    "     'time_gap_before_total',\n",
    "     'time_gap_after_total',\n",
    "    #  2_3处理后的特征\n",
    "    #  生成用户对属性全局点击时间差特征\n",
    "    #   ['item_id', 'item_brand_id', 'shop_id', 'context_page_id', 'category2_label',]    \n",
    "#     'user_item_id_first_click',\n",
    "#     'user_item_id_last_click',\n",
    "#     'user_item_id_click_rank',\n",
    "     'user_item_id_first_click_interval',\n",
    "     'user_item_id_last_click_interval',\n",
    "#     'user_item_id_diff_click_interval',\n",
    "#     'user_item_id_prob_click_interval',\n",
    "     'item_id_time_gap_before',\n",
    "     'item_id_time_gap_after',\n",
    "#     'user_item_id_click_true_rank',\n",
    "#     'user_item_brand_id_first_click',\n",
    "#     'user_item_brand_id_last_click',\n",
    "#     'user_item_brand_id_click_rank',\n",
    "     'user_item_brand_id_first_click_interval',\n",
    "     'user_item_brand_id_last_click_interval',\n",
    "#     'user_item_brand_id_diff_click_interval',\n",
    "#     'user_item_brand_id_prob_click_interval',\n",
    "     'item_brand_id_time_gap_before',\n",
    "     'item_brand_id_time_gap_after',\n",
    "#     'user_item_brand_id_click_true_rank',\n",
    "#     'user_shop_id_first_click',\n",
    "#     'user_shop_id_last_click',\n",
    "#     'user_shop_id_click_rank',\n",
    "     'user_shop_id_first_click_interval',\n",
    "     'user_shop_id_last_click_interval',\n",
    "#     'user_shop_id_diff_click_interval',\n",
    "#     'user_shop_id_prob_click_interval',\n",
    "     'shop_id_time_gap_before',\n",
    "     'shop_id_time_gap_after',\n",
    "#     'user_shop_id_click_true_rank',\n",
    "#      'user_context_page_id_first_click',\n",
    "#      'user_context_page_id_last_click',\n",
    "#      'user_context_page_id_click_rank',\n",
    "#      'user_context_page_id_first_click_interval',\n",
    "#      'user_context_page_id_last_click_interval',\n",
    "# #     'user_context_page_id_diff_click_interval',\n",
    "# #     'user_context_page_id_prob_click_interval',\n",
    "#      'context_page_id_time_gap_before',\n",
    "#      'context_page_id_time_gap_after',\n",
    "#      'user_context_page_id_click_true_rank',\n",
    "#     'user_category2_label_first_click',\n",
    "#     'user_category2_label_last_click',\n",
    "#     'user_category2_label_click_rank',\n",
    "     'user_category2_label_first_click_interval',\n",
    "     'user_category2_label_last_click_interval',\n",
    "#     'user_category2_label_diff_click_interval',\n",
    "#     'user_category2_label_prob_click_interval',\n",
    "     'category2_label_time_gap_before',\n",
    "     'category2_label_time_gap_after',\n",
    "#     'user_category2_label_click_true_rank',\n",
    "    #  2_3处理后的特征\n",
    "    #  生成用户对属性当天点击时间差特征\n",
    "    #   ['item_id', 'item_brand_id', 'shop_id', 'context_page_id', 'category2_label',]    \n",
    "#     'user_item_id_first_click_day',\n",
    "#     'user_item_id_last_click_day',\n",
    "     'user_item_id_click_rank_day',\n",
    "     'user_item_id_first_click_interval_day',\n",
    "     'user_item_id_last_click_interval_day',\n",
    "#     'user_item_id_diff_click_interval_day',\n",
    "#     'user_item_id_prob_click_interval_day',\n",
    "     'item_id_time_gap_before_day',\n",
    "     'item_id_time_gap_after_day',\n",
    "     'user_item_id_click_true_rank_day',\n",
    "#     'user_item_brand_id_first_click_day',\n",
    "#     'user_item_brand_id_last_click_day',\n",
    "     'user_item_brand_id_click_rank_day',\n",
    "     'user_item_brand_id_first_click_interval_day',\n",
    "     'user_item_brand_id_last_click_interval_day',\n",
    "#     'user_item_brand_id_diff_click_interval_day',\n",
    "#     'user_item_brand_id_prob_click_interval_day',\n",
    "     'item_brand_id_time_gap_before_day',\n",
    "     'item_brand_id_time_gap_after_day',\n",
    "     'user_item_brand_id_click_true_rank_day',\n",
    "#     'user_shop_id_first_click_day',\n",
    "#     'user_shop_id_last_click_day',\n",
    "     'user_shop_id_click_rank_day',\n",
    "     'user_shop_id_first_click_interval_day',\n",
    "     'user_shop_id_last_click_interval_day',\n",
    "#     'user_shop_id_diff_click_interval_day',\n",
    "#     'user_shop_id_prob_click_interval_day',\n",
    "     'shop_id_time_gap_before_day',\n",
    "     'shop_id_time_gap_after_day',\n",
    "     'user_shop_id_click_true_rank_day',\n",
    "#      'user_context_page_id_first_click_day',\n",
    "#      'user_context_page_id_last_click_day',\n",
    "#      'user_context_page_id_click_rank_day',\n",
    "#      'user_context_page_id_first_click_interval_day',\n",
    "#      'user_context_page_id_last_click_interval_day',\n",
    "# #     'user_context_page_id_diff_click_interval_day',\n",
    "# #     'user_context_page_id_prob_click_interval_day',\n",
    "#      'context_page_id_time_gap_before_day',\n",
    "#      'context_page_id_time_gap_after_day',\n",
    "#      'user_context_page_id_click_true_rank_day',\n",
    "#     'user_category2_label_first_click_day',\n",
    "#     'user_category2_label_last_click_day',\n",
    "     'user_category2_label_click_rank_day',\n",
    "     'user_category2_label_first_click_interval_day',\n",
    "     'user_category2_label_last_click_interval_day',\n",
    "#     'user_category2_label_diff_click_interval_day',\n",
    "#     'user_category2_label_prob_click_interval_day',\n",
    "     'category2_label_time_gap_before_day',\n",
    "     'category2_label_time_gap_after_day',\n",
    "     'user_category2_label_click_true_rank_day',\n",
    "    \n",
    "    #  2_4处理后的特征\n",
    "     'property_sim',\n",
    "     'category_predict_rank',\n",
    "     'category_3',\n",
    "    \n",
    "    #  2_5处理后的特征\n",
    "    #  生成单特征历史点击率，要去除点击次数和点击时间\n",
    "    #     ['user_id', 'category_predict_rank', 'user_occupation_id', 'user_age_level', 'user_gender_id', 'user_star_level',\n",
    "    #     'item_id', 'item_brand_id', 'item_city_id', 'category2_label','item_price_level','item_sales_level',\n",
    "    #     'item_collected_level', 'item_pv_level',\n",
    "    #     'context_page_id',\n",
    "    #     'shop_id', 'shop_review_num_level', 'shop_star_level',]    \n",
    "     'user_id_smooth_CTR',\n",
    "     'category_predict_rank_smooth_CTR',\n",
    "     'user_occupation_id_smooth_CTR',\n",
    "     'user_age_level_smooth_CTR',\n",
    "     'user_gender_id_smooth_CTR',\n",
    "     'user_star_level_smooth_CTR',\n",
    "     'item_id_smooth_CTR',\n",
    "     'item_brand_id_smooth_CTR',\n",
    "#     'item_city_id_smooth_CTR',\n",
    "     'category2_label_smooth_CTR',\n",
    "     'item_price_level_smooth_CTR',\n",
    "     'item_sales_level_smooth_CTR',\n",
    "     'item_collected_level_smooth_CTR',\n",
    "#     'item_pv_level_smooth_CTR',\n",
    "     'context_page_id_smooth_CTR',\n",
    "     'shop_id_smooth_CTR',\n",
    "#     'shop_review_num_level_smooth_CTR',\n",
    "#     'shop_star_level_smooth_CTR',\n",
    "    #  2_5处理后的特征\n",
    "    #  生成单特征前一天点击率，前一天的点击次数，前一天的购买次数， 去除ctr\n",
    "    #     ['user_id', 'category_predict_rank', 'user_occupation_id', 'user_age_level', 'user_gender_id', 'user_star_level',\n",
    "    #     'item_id', 'item_brand_id', 'item_city_id', 'category2_label','item_price_level','item_sales_level',\n",
    "    #     'item_collected_level', 'item_pv_level',\n",
    "    #     'context_page_id',\n",
    "    #     'shop_id', 'shop_review_num_level', 'shop_star_level',]\n",
    "     'user_id_day_I',\n",
    "     'user_id_day_C',\n",
    "#     'category_predict_rank_day_I',\n",
    "#     'category_predict_rank_day_C',\n",
    "#     'user_occupation_id_day_I',\n",
    "#     'user_occupation_id_day_C',\n",
    "#     'user_age_level_day_I',\n",
    "#     'user_age_level_day_C',\n",
    "#     'user_gender_id_day_I',\n",
    "#     'user_gender_id_day_C',\n",
    "#     'user_star_level_day_I',\n",
    "#     'user_star_level_day_C',\n",
    "     'item_id_day_I',\n",
    "     'item_id_day_C',\n",
    "     'item_brand_id_day_I',\n",
    "     'item_brand_id_day_C',\n",
    "#     'item_city_id_day_I',\n",
    "#     'item_city_id_day_C',\n",
    "     'category2_label_day_I',\n",
    "     'category2_label_day_C',\n",
    "#     'item_price_level_day_I',\n",
    "#     'item_price_level_day_C',\n",
    "#     'item_sales_level_day_I',\n",
    "#     'item_sales_level_day_C',\n",
    "#     'item_collected_level_day_I',\n",
    "#     'item_collected_level_day_C',\n",
    "#     'item_pv_level_day_I',\n",
    "#     'item_pv_level_day_C',\n",
    "#     'context_page_id_day_I',\n",
    "#     'context_page_id_day_C',\n",
    "     'shop_id_day_I',\n",
    "     'shop_id_day_C',\n",
    "#     'shop_review_num_level_day_I',\n",
    "#     'shop_review_num_level_day_C',\n",
    "#     'shop_star_level_day_I',\n",
    "#     'shop_star_level_day_C',\n",
    "\n",
    "    #  2_5处理后的特征\n",
    "    #  生成历史交叉点击率，前一天的点击次数，前一天的购买次数，只保留ctr\n",
    "    #    ['user_gender_id', 'user_age_level', 'user_occupation_id']\n",
    "    #    ['item_id', 'item_brand_id', 'shop_id']    \n",
    "\n",
    "     'user_gender_id_item_id_smooth_CTR',\n",
    "     'user_gender_id_item_brand_id_smooth_CTR',\n",
    "     'user_gender_id_shop_id_smooth_CTR',\n",
    "     'user_gender_id_item_price_level_smooth_CTR',\n",
    "     'user_age_level_item_id_smooth_CTR',\n",
    "     'user_age_level_item_brand_id_smooth_CTR',\n",
    "     'user_age_level_shop_id_smooth_CTR',\n",
    "     'user_age_level_item_price_level_smooth_CTR',\n",
    "     'user_occupation_id_item_id_smooth_CTR',\n",
    "     'user_occupation_id_item_brand_id_smooth_CTR',\n",
    "     'user_occupation_id_shop_id_smooth_CTR',\n",
    "     'user_occupation_id_item_price_level_smooth_CTR',\n",
    "#     'user_star_level_item_id_smooth_CTR',\n",
    "#     'user_star_level_item_brand_id_smooth_CTR',\n",
    "#     'user_star_level_shop_id_smooth_CTR',\n",
    "     'user_star_level_item_price_level_smooth_CTR',\n",
    "\n",
    "    \n",
    "    #  2_6处理后的特征\n",
    "    #  分别groupby['shop_id', 'item_id', 'item_brand_id', 'item_price_level']\n",
    "    #    计算item在['user_gender_id', 'user_age_level', 'user_occupation_id', 'user_star_level']几个属性下的点击量\n",
    "     'shop_id_user_gender_id_click_rate',\n",
    "     'shop_id_user_age_level_click_rate',\n",
    "     'shop_id_user_occupation_id_click_rate',\n",
    "#     'shop_id_user_star_level_click_rate',\n",
    "     'item_id_user_gender_id_click_rate',\n",
    "     'item_id_user_age_level_click_rate',\n",
    "     'item_id_user_occupation_id_click_rate',\n",
    "#     'item_id_user_star_level_click_rate',\n",
    "     'item_brand_id_user_gender_id_click_rate',\n",
    "     'item_brand_id_user_age_level_click_rate',\n",
    "     'item_brand_id_user_occupation_id_click_rate',\n",
    "#     'item_brand_id_user_star_level_click_rate',\n",
    "     'item_price_level_user_gender_id_click_rate',\n",
    "     'item_price_level_user_age_level_click_rate',\n",
    "     'item_price_level_user_occupation_id_click_rate',\n",
    "     'item_price_level_user_star_level_click_rate',\n",
    "    \n",
    "    #  2_7处理后的特征\n",
    "    #  计算每天的点击量\n",
    "    #     ['user_id', 'user_occupation_id', 'user_age_level', 'user_gender_id', 'user_star_level',\n",
    "    #      'item_id', 'item_brand_id', 'item_city_id', 'category2_label','item_price_level','item_sales_level',\n",
    "    #      'item_collected_level', 'item_pv_level',\n",
    "    #      'context_page_id',\n",
    "    #      'shop_id', 'shop_review_num_level', 'shop_star_level',]\n",
    "     'user_id_click_day',\n",
    "     'user_occupation_id_click_day',\n",
    "     'user_age_level_click_day',\n",
    "     'user_gender_id_click_day',\n",
    "     'user_star_level_click_day',\n",
    "     'item_id_click_day',\n",
    "     'item_brand_id_click_day',\n",
    "#     'item_city_id_click_day',\n",
    "     'category2_label_click_day',\n",
    "     'item_price_level_click_day',\n",
    "     'item_sales_level_click_day',\n",
    "     'item_collected_level_click_day',\n",
    "#     'item_pv_level_click_day',\n",
    "     'context_page_id_click_day',\n",
    "     'shop_id_click_day',\n",
    "#     'shop_review_num_level_click_day',\n",
    "#     'shop_star_level_click_day',\n",
    "    #  2_7处理后的特征\n",
    "    #  计算每天每小时的点击量\n",
    "    #     ['user_id', 'user_occupation_id', 'user_age_level', 'user_gender_id', 'user_star_level',\n",
    "    #      'item_id', 'item_brand_id', 'item_city_id', 'category2_label','item_price_level','item_sales_level',\n",
    "    #      'item_collected_level', 'item_pv_level',\n",
    "    #      'context_page_id',\n",
    "    #      'shop_id', 'shop_review_num_level', 'shop_star_level',]\n",
    "     'user_id_click_hour_x',\n",
    "     'user_occupation_id_click_hour_x',\n",
    "     'user_age_level_click_hour_x',\n",
    "     'user_gender_id_click_hour_x',\n",
    "     'user_star_level_click_hour_x',\n",
    "     'item_id_click_hour_x',\n",
    "     'item_brand_id_click_hour_x',\n",
    "#     'item_city_id_click_hour_x',\n",
    "     'category2_label_click_hour_x',\n",
    "     'item_price_level_click_hour_x',\n",
    "     'item_sales_level_click_hour_x',\n",
    "     'item_collected_level_click_hour_x',\n",
    "#     'item_pv_level_click_hour_x',\n",
    "#     'context_page_id_click_hour_x',\n",
    "     'shop_id_click_hour_x',\n",
    "#     'shop_review_num_level_click_hour_x',\n",
    "#     'shop_star_level_click_hour_x',\n",
    "    #  2_7处理后的特征\n",
    "    #  计算每小时的点击量\n",
    "    #     ['user_id', 'user_occupation_id', 'user_age_level', 'user_gender_id', 'user_star_level',\n",
    "    #      'item_id', 'item_brand_id', 'item_city_id', 'category2_label','item_price_level','item_sales_level',\n",
    "    #      'item_collected_level', 'item_pv_level',\n",
    "    #      'context_page_id',\n",
    "    #      'shop_id', 'shop_review_num_level', 'shop_star_level',]\n",
    "     'user_id_click_hour_y',\n",
    "     'user_occupation_id_click_hour_y',\n",
    "     'user_age_level_click_hour_y',\n",
    "     'user_gender_id_click_hour_y',\n",
    "#     'user_star_level_click_hour_y',\n",
    "     'item_id_click_hour_y',\n",
    "     'item_brand_id_click_hour_y',\n",
    "#     'item_city_id_click_hour_y',\n",
    "     'category2_label_click_hour_y',\n",
    "#     'item_price_level_click_hour_y',\n",
    "#     'item_sales_level_click_hour_y',\n",
    "#     'item_collected_level_click_hour_y',\n",
    "#     'item_pv_level_click_hour_y',\n",
    "#     'context_page_id_click_hour_y',\n",
    "     'shop_id_click_hour_y',\n",
    "#     'shop_review_num_level_click_hour_y',\n",
    "#     'shop_star_level_click_hour_y'\n",
    "]\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.674789\tval-logloss:0.674734\n",
      "Multiple eval metrics have been passed: 'val-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until val-logloss hasn't improved in 200 rounds.\n",
      "[50]\ttrain-logloss:0.238424\tval-logloss:0.236086\n",
      "[100]\ttrain-logloss:0.128134\tval-logloss:0.124244\n",
      "[150]\ttrain-logloss:0.096399\tval-logloss:0.091686\n",
      "[200]\ttrain-logloss:0.087346\tval-logloss:0.082471\n",
      "[250]\ttrain-logloss:0.084415\tval-logloss:0.079883\n",
      "[300]\ttrain-logloss:0.082987\tval-logloss:0.078975\n",
      "[350]\ttrain-logloss:0.082004\tval-logloss:0.078608\n",
      "[400]\ttrain-logloss:0.081177\tval-logloss:0.078427\n",
      "[450]\ttrain-logloss:0.080461\tval-logloss:0.078311\n",
      "[500]\ttrain-logloss:0.079804\tval-logloss:0.078245\n",
      "[550]\ttrain-logloss:0.079223\tval-logloss:0.078234\n",
      "[600]\ttrain-logloss:0.078648\tval-logloss:0.078253\n",
      "[650]\ttrain-logloss:0.078089\tval-logloss:0.07824\n",
      "[700]\ttrain-logloss:0.077584\tval-logloss:0.078235\n",
      "[750]\ttrain-logloss:0.077127\tval-logloss:0.078236\n",
      "[800]\ttrain-logloss:0.076721\tval-logloss:0.078264\n",
      "[850]\ttrain-logloss:0.076264\tval-logloss:0.078282\n",
      "Stopping. Best iteration:\n",
      "[662]\ttrain-logloss:0.077972\tval-logloss:0.07823\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "import xgboost as xgb\n",
    "train_data = all_data[(all_data.day >= 19) & (all_data.day <= 23)]\n",
    "test_data = all_data[all_data.day == 24]\n",
    "\n",
    "dtrain = xgb.DMatrix(train_data[features], train_data[target])\n",
    "dtest = xgb.DMatrix(test_data[features], test_data[target])\n",
    "\n",
    "watchlist = [(dtrain, 'train'), (dtest, 'val')]\n",
    "\n",
    "params = {\n",
    "    'n_estimators': 2000,\n",
    "    'max_depth': 5,\n",
    "    'eta': 0.02,\n",
    "    'eval_metric': 'logloss',\n",
    "    'objective': 'binary:logistic',\n",
    "    'subsample': 1.0,\n",
    "    'colsample_bytree': 0.7,\n",
    "#     'random_state': 1123,\n",
    "#     'min_child_weight': 10\n",
    "    #'scale_pos_weight':0.5\n",
    "}\n",
    "\n",
    "xgb_a = xgb.train(params, dtrain,\n",
    "                  num_boost_round=1000,\n",
    "                  early_stopping_rounds=200,\n",
    "                  evals=watchlist,\n",
    "                  verbose_eval=50)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files (x86)\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.07615493807464839, 0.07828151363584884)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_train = log_loss(train_data[target], xgb_a.predict(dtrain))\n",
    "loss_test = log_loss(test_data[target], xgb_a.predict(dtest))\n",
    "test_data['predicted_score'] = xgb_a.predict(dtest)\n",
    "test_data[['instance_id', 'predicted_score']].to_csv(\n",
    "    '24_226_xgboost.txt', index=False, sep=' ')\n",
    "\n",
    "loss_train, loss_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "train_data = all_data[(all_data.day >= 19) & (all_data.day <= 23)]\n",
    "test_data = all_data[all_data.day == 24]\n",
    "\n",
    "lgb_clf = lgb.LGBMClassifier(objective='binary',\n",
    "\n",
    "                             n_estimators=2000,\n",
    "                             learning_rate=0.02,\n",
    "\n",
    "                             max_depth=4,\n",
    "                             num_leaves=15,\n",
    "                             min_child_samples=70,\n",
    "                             min_child_weight=1e-3,\n",
    "\n",
    "                             colsample_bytree=0.9,\n",
    "                             subsample=0.7,\n",
    "                             subsample_freq=1,\n",
    "\n",
    "                             reg_lambda=12,\n",
    "                             min_split_gain=0.,\n",
    "\n",
    "                             n_jobs=-1,\n",
    "                             silent=False\n",
    "                             )\n",
    "\n",
    "\n",
    "#cate_features = ['user_gender_id', 'user_occupation_id', 'hour']\n",
    "\n",
    "lgb_clf.fit(train_data[features], train_data[target],\n",
    "            eval_set=[(test_data[features], test_data[target])],\n",
    "            early_stopping_rounds=200,\n",
    "            feature_name=features,\n",
    "#             categorical_feature=cate_features,\n",
    "            verbose=50,\n",
    "            )\n",
    "\n",
    "loss_train = log_loss(train_data[target],lgb_clf.predict_proba(train_data[features]))\n",
    "loss_test = log_loss(test_data[target], lgb_clf.predict_proba(test_data[features]))\n",
    "\n",
    "\n",
    "loss_train, loss_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[50]\tvalid_0's binary_logloss: 0.241304\n",
      "[100]\tvalid_0's binary_logloss: 0.127046\n",
      "[150]\tvalid_0's binary_logloss: 0.0936575\n",
      "[200]\tvalid_0's binary_logloss: 0.0840902\n",
      "[250]\tvalid_0's binary_logloss: 0.0813023\n",
      "[300]\tvalid_0's binary_logloss: 0.0803174\n",
      "[350]\tvalid_0's binary_logloss: 0.0798454\n",
      "[400]\tvalid_0's binary_logloss: 0.0795709\n",
      "[450]\tvalid_0's binary_logloss: 0.0793839\n",
      "[500]\tvalid_0's binary_logloss: 0.0792878\n",
      "[550]\tvalid_0's binary_logloss: 0.0791922\n",
      "[600]\tvalid_0's binary_logloss: 0.0791346\n",
      "[650]\tvalid_0's binary_logloss: 0.0790822\n",
      "[700]\tvalid_0's binary_logloss: 0.0790708\n",
      "[750]\tvalid_0's binary_logloss: 0.0790699\n",
      "[800]\tvalid_0's binary_logloss: 0.0790485\n",
      "[850]\tvalid_0's binary_logloss: 0.079025\n",
      "[900]\tvalid_0's binary_logloss: 0.0790363\n",
      "[950]\tvalid_0's binary_logloss: 0.079021\n",
      "[1000]\tvalid_0's binary_logloss: 0.0790158\n",
      "[1050]\tvalid_0's binary_logloss: 0.0789942\n",
      "[1100]\tvalid_0's binary_logloss: 0.0789948\n",
      "[1150]\tvalid_0's binary_logloss: 0.0789952\n",
      "[1200]\tvalid_0's binary_logloss: 0.0790156\n",
      "[1250]\tvalid_0's binary_logloss: 0.0790252\n",
      "[1300]\tvalid_0's binary_logloss: 0.0790496\n",
      "Early stopping, best iteration is:\n",
      "[1120]\tvalid_0's binary_logloss: 0.0789821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files (x86)\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.08026625494912704, 0.07898206400049905)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "train_data = all_data[(all_data.day >= 19) & (all_data.day <= 22)]\n",
    "test_data = all_data[all_data.day == 23]\n",
    "\n",
    "lgb_clf = lgb.LGBMClassifier(objective='binary',\n",
    "\n",
    "                             n_estimators=2000,\n",
    "                             learning_rate=0.02,\n",
    "\n",
    "                             max_depth=4,\n",
    "                             num_leaves=15,\n",
    "                             min_child_samples=100,\n",
    "                             min_child_weight=1e-3,\n",
    "\n",
    "                             colsample_bytree=1.0,\n",
    "                             subsample=0.7,\n",
    "                             subsample_freq=1,\n",
    "\n",
    "                             reg_lambda=15,\n",
    "                             min_split_gain=0.,\n",
    "                             \n",
    "                             max_bin=63,\n",
    "\n",
    "                             n_jobs=-1,\n",
    "                             silent=False,\n",
    "                             \n",
    "                             #device='gpu',\n",
    "                             gpu_use_dp=False,\n",
    "                             )\n",
    "\n",
    "lgb_clf.fit(train_data[features], train_data[target],\n",
    "            eval_set=[(test_data[features], test_data[target])],\n",
    "            early_stopping_rounds=200,\n",
    "            feature_name=features,\n",
    "#             categorical_feature=cate_features,\n",
    "            verbose=50,\n",
    "            )\n",
    "\n",
    "loss_train = log_loss(train_data[target],lgb_clf.predict_proba(train_data[features]))\n",
    "loss_test = log_loss(test_data[target], lgb_clf.predict_proba(test_data[features]))\n",
    "\n",
    "test_data['predicted_score'] = lgb_clf.predict_proba(test_data[features])[:, 1]\n",
    "\n",
    "test_data[['instance_id', 'predicted_score']].to_csv(\n",
    "    '23_226.txt', index=False, sep=' ')\n",
    "\n",
    "\n",
    "loss_train, loss_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "19-22 23  1120 0.0802662 0.078982     950:0.079021      1300:0.079049  \n",
    "\n",
    "19-23 24  1207 0.0794017 0.077929     900:0.07799       1400:0.0779677\n",
    "20-23 24  900  0.0792304 0.078160     750:0.07819       1100:0.0781925\n",
    "21-23 24  768  0.0782709 0.078262     750:0.07827       950:0.0782948\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(399850, 472)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgb\n",
    "train_data = all_data[(all_data.day >= 19) & (all_data.day <= 24)]\n",
    "print(train_data.shape)\n",
    "\n",
    "test_data = all_data[(all_data.day == 25) & (all_data.is_trade == -2)]\n",
    "\n",
    "lgb_clf = lgb.LGBMClassifier(objective='binary',\n",
    "\n",
    "                             n_estimators=1340,\n",
    "                             learning_rate=0.02,\n",
    "\n",
    "                             max_depth=4,\n",
    "                             num_leaves=15,\n",
    "                             min_child_samples=100,\n",
    "                             min_child_weight=1e-3,\n",
    "\n",
    "                             colsample_bytree=1.0,\n",
    "                             subsample=0.7,\n",
    "                             subsample_freq=1,\n",
    "\n",
    "                             reg_lambda=15,\n",
    "                             min_split_gain=0.,\n",
    "                             \n",
    "                             max_bin=63,\n",
    "\n",
    "                             n_jobs=-1,\n",
    "                             silent=False,\n",
    "                             \n",
    "                             #device='gpu',\n",
    "                             gpu_use_dp=False,\n",
    "                             )\n",
    "\n",
    "                     \n",
    "lgb_clf.fit(train_data[features], train_data[target],feature_name=features)\n",
    "\n",
    "loss_train = log_loss(train_data[target], lgb_clf.predict_proba(train_data[features]))\n",
    "\n",
    "test_data['predicted_score'] = lgb_clf.predict_proba(test_data[features])[:, 1]\n",
    "\n",
    "test_data[['instance_id', 'predicted_score']].to_csv(\n",
    "    '20180421_1340.txt', index=False, sep=' ')\n",
    "\n",
    "loss_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = pd.DataFrame(bst.feature_importance())\n",
    "importance.columns = ['importance_20']\n",
    "features = pd.DataFrame(features)\n",
    "features.columns = ['features_20']\n",
    "pd.set_option('max_rows',500)\n",
    "merge = features.join(importance)\n",
    "merge = merge.sort_values(by=['importance_20'], ascending=False).reset_index()\n",
    "merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07898206400049905\n",
      "0.07896555775084205\n",
      "0.07895825162791338\n",
      "0.07895952163050796\n",
      "0.07896894320019182\n",
      "0.07898621946888613\n",
      "0.07901114730307535\n",
      "0.07904359967726714\n",
      "0.07908351654637066\n",
      "0.07913090101163017\n",
      "0.0791858194837445\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "train_data = all_data[(all_data.day >= 19) & (all_data.day <= 22)]\n",
    "test_data = all_data[all_data.day == 23]\n",
    "qwcxg_predict = pd.read_csv(\"24_226_xgboost.txt\", sep=' ')['predicted_score']\n",
    "qwc_predict = pd.read_csv(\"25_226.txt\", sep=' ')['predicted_score']\n",
    "yym = pd.read_csv(\"features-273-depth-5-with_25.txt\", sep=' ')['predicted_score']\n",
    "yym_23 = pd.read_csv(\"yym_23.txt\", sep=' ')['predicted_score']\n",
    "qwc_23 = pd.read_csv(\"23_226.txt\", sep=' ')['predicted_score']\n",
    "for i in range(11):\n",
    "    w1 = 0.1*i\n",
    "    w2 = 1- w1\n",
    "    total = yym_23*w1+qwc_23*w2\n",
    "    loss_test = log_loss(test_data[target],total)\n",
    "    print(loss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
